---
author_1: "Gabriel Strain"
author_2: "Andrew J. Stewart"
author_3: "Paul Warren"
author_4: "Caroline Jay"
author_1_email: "Gabriel.Strain@manchester.ac.uk"
author_2_email: "Andrew.J.Stewart@manchester.ac.uk"
author_3_email: "Paul.Warren@manchester.ac.uk"
author_4_email: "Caroline.Jay@manchester.ac.uk"
affiliation: "The University of Manchester"
acknowledgements: "This work was supported by funding from the University of Manchester Department of Computer Science and Division of Psychology, Communication and Human Neuroscience."
output:
  bookdown::pdf_book: # for automatic figure-numbering (https://bookdown.org/yihui/rmarkdown-cookbook/figure-number.html)
    keep_tex: yes
    template: template.tex
    citation_package: natbib
    extra_dependencies: ["float"]
title: "A Novel Technique to Facilitate More Accurate Correlation Perception in Scatterplots"
editor_options: 
  markdown: 
    wrap: 72
bibliography: size-and-scatterplots
abstract: |
  Viewers consistently underestimate correlation in positively correlated scatterplots.
  We use a novel point size manipulation to correct for this bias. In a high-powered and
  fully reproducible study, we demonstrate that decreasing the size of a point in a scatterplot
  as a function of its distance from the regression line is able to correct for a systematic
  perceptual bias long present in the literature. We suggest the implementation of our technique
  when designing scatterplots that aim to communicate correlation.
introduction: |
---
```{r setup, include=FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Knitting this document requires tinytex (install.packages("tinytex"))
# For formatting to be correct, additional tinytex packages are required
# Run tinytex:::install_yihui_pkgs() before knitting
```

```{r, libraries-and-conflicts, include=FALSE}
set.seed(1234) # seed for all random number generation

# Loading packages
library(rticles)
library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(broom.mixed)
library(insight)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(pwr)
library(conflicted)
library(EMAtools)

# tell R to use dplyr::select() instead of MASS::select()

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())

# Comment out the following line if additional tinytex packages are already installed
# If building this in dockerised Rstudio instance, run this command the first time

# tinytex:::install_yihui_pkgs()
```

```{r eval-models, include=FALSE}
# in this script, models are cached. If eval_models <- FALSE, script will load
# cached models. Set eval_models <- TRUE to rebuild models from scratch

eval_models <- FALSE

if (eval_models == FALSE){
  lazyload_cache_dir('size_and_scatterplots/latex')
}
```

```{r load-data, include=FALSE}
# load in data files

size_anon <- read_csv("data/final_data.csv")
```

```{r wrangle, include = FALSE}
# function for wrangling data

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
  visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels, pattern = "vis_threshold_plots/", replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer, pattern = "_VT.png", replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    select("VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
  monitor_information <- anon_file %>%
    filter(!is.na(height)) %>%
    filter(!is.na(res_width)) %>%
    mutate(res_height = res_width*0.5625,
           width = height*0.5625,
           dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
        select(c("dot_pitch", "participant", "res_width"))
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split plots_with_labels column into item and contrast condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "size"), sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session",
                  "trials.thisN")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "size")), as_factor)) %>%
  select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(size = fct_relevel(size, c('D', 'C', 'B', 'A'))) %>% 
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data files 

wrangle(size_anon)

# remove anon df from environment

rm(size_anon)

# extract age data

age <- distinct(exp_size_only_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data

gender <- distinct(exp_size_only_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(exp_size_only_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))



```

```{r comparison-function, include=FALSE}
# this function takes a model and creates a nested model with the fixed effects 
# term removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r anova-results-function, include=FALSE}
# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r contrasts-extract, echo=FALSE}
# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size)
  
  params <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(params)
  
}
```

```{r effect-size-function, include = FALSE}
get_effects_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r dot-plot-function, include = FALSE}
dot_plot_function <- function(df) {

data <- df %>%
  group_by(size) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(size)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1],
    hci = t.test(difference, conf.level = 0.95)$conf.int[2],
  )

  data %>%
    mutate(size = fct_relevel(size, "D", "C", "B", "A")) %>%
    ggplot(aes(x = size, y = mean)) +
    geom_point(stat = "identity", size = 1) +
    #coord_cartesian(ylim = c(0, max_error))
    geom_errorbar(aes(ymin=lci, ymax=hci), colour="black", width=0.01, linewidth =0.5) +
    theme_ggdist() +
    labs(x = "Point Size Condition",
         y = "Mean Error") +
    theme(axis.text = element_text(size = 13),
          axis.title = element_text(size = 16))
}
```

```{r error-bar-plot, include = FALSE}
plot_error_bars_function <- function(df, measure, l){
  df %>% 
  drop_na() %>% 
  group_by(size, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd),width = 0.01, size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(0,1, 0.2)) +
  theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 6.5),
        axis.title = element_text(size = 8)) +
  facet_wrap(size ~., ncol = 4, labeller = labeller(size = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}

```

```{r labeller, include = FALSE}
labels_size <- c(A = "Non-Linear Decay", B = "Linear Decay", C = "Inverted Decay", D = "Standard Size")
```

```{r example-plots, include = FALSE}
example_plots <- function () {
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_linear = my_residuals/3.2) %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1)
  
plot_example_function <- function (d, x, t) {
  
  set.seed(1234)
  
  ggplot(d, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(x + 0.2)), shape = 16)  +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 15)) +
  labs(title = t)

}  

plots <- ggarrange(plot_example_function(slopes, (1-slopes$slope_0.25), "Non-linear Decay (b = 0.25)"),
                   plot_example_function(slopes, (1-slopes$slope_linear), "Linear Decay"),
                   plot_example_function(slopes, (1-slopes$slope_inverted), "Inverted Non-linear Decay"),
                   plot_example_function(slopes, 0.05, "Standard Size"))

return(plots)

}
```

# Introduction

Scatterplots, utilized in scientific communication for a variety of tasks,
are some of the most widely used and studied data visualizations. Viewers
interpret them in similar ways \cite{kay_heer_2015}, and they are simple
enough to be easily studied while providing important insights into visualization
design, human-computer interaction, and human perception. In a previous study \cite{strain_2023},
we showed that a novel point contrast manipulation, in which the contrast of a certain
scatterplot point was reduced as the size of that point's residual increased, could be
used to partially correct for a systematic correlation underestimation bias present in the 
literature \cite{strahan_1978, bobko_1979, cleveland_1984, lane_1985, lauer_1989, 
collyer_1990, meyer_1992}. We suggested that this was due to a narrowing of the width
of the perceived probability distribution of a plot brought
about by the lower contrast (and therefore lower point-salience and higher spatial uncertainty) in those outer areas. In that study we tested linear, non-linear, and non-linear inverted functions relating
point contrast to residual size, finding that the non-linear function produced
the most accurate estimates of correlation, and that the non-linear inverted produced 
the least accurate. In the present study we use the same equations to manipulate
point size.

## Scatterplots and Correlation

Scatterplots have been widely studied, especially as mediums for the communication
of correlation (see \cite{strain_2023} for a review of the history of this work).
Previous literature has found evidence for a pronounced underestimation in judgements
of correlation in positively correlated scatterplots, especially between 0.2 < *r* < 0.6. The
nature of this investigation has varied, ranging from direct estimation,
to discriminative judgement, bisection, and staircase tasks. As in our previous work,
we use the direct estimation paradigm owing to its simplicity and its suitability
to online experimentation. This renders the judgements we collect
comparative by nature, although such work does allow us to inform design guidelines as
well as human perception. It is our duty as visualization designers to ensure that the
messages we are trying to communicate are being interpreted as accurately as possible
by viewers. To achieve this, we must understand human perception, apply that understanding
to design, and test those designs in rigorous empirical studies.

## Point Size

Point contrast is not the only available visual feature that might be used
to influence viewers' perceptions of the width of a probability distribution, neither is
it the only visual feature of a scatterplot that we can exploit. While
contrast adjustments have been used extensively to solve issues of overplotting and clutter
in scatterplots \cite{matejka_2015, bertini_2004}, there is no established use for
varying point size. Common sense dictates that scatterplots visualizing larger
datasets inherently require their points to be smaller to prevent obfuscation of the data,
but to our knowledge there is little testing of the impact of point size on correlation perception.
Studies have found invariance in the bias and variability of correlation 
perception with regards to changing point sizes,
but these have been low-powered \cite{rensink_2012, rensink_2014}. From the wider literature there is evidence that larger points are more salient \cite{healey_2012}, can bias judgements of point position more strongly than point contrast can \cite{hong_2021}, and can result in faster reaction times to peripherally presented stimuli \cite{grice_1983}. In addition, smaller stimuli are associated with greater levels of spatial uncertainty \cite{alais_2004}, and if this 
is driving the reduction in bias we saw in our previous work \cite{strain_2023},
we would expect a similar effect when point size is used instead of point contrast.

## Hypotheses

We hypothesize that correlation estimates will be most accurate when
viewers are presented with the non-linear size decay condition, and will be 
least accurate when presented with the non-linear inverted size decay condition.
We thereby present a single experiment study in which we demonstrate that the use of 
a non-linear size decay function relating to the residuals of points on scatterplots
can be employed to correct for a systematic underestimation of correlation by 
viewers. We find no evidence for effects of graph literacy or training.
The effect we observe here is much stronger, both
with regards to effect size and in terms of the observed reduction in error, than that
observed in our previous study \cite{strain_2023}. We suggest that this 
function can be used to facilitate more accurate correlation
perception in scatterplots, and provide exciting future avenues for the continuation
and refinement of these techniques. Ethical approval was granted by the University
of Manchester's Computer Science Departmental Panel (Ref: 2022-14660-24397).

# Methodology

The experiment was conducted according to the principles of open and reproducible research.
All data and analysis code are available at https://github.com/gjpstrain/size_and_scatterplots.
This repository contains instructions for building a docker image to fully 
reproduce the computational environment used, allowing for full replications
of stimulus generation, analyses, and the paper itself. The experiment was 
pre-registered with the OSF (https://osf.io/k4gd8).

## Participants

150 participants were recruited using the Prolific.co platform. Normal to
corrected-to-normal vision and English fluency were required for participation. As in our previous work
\cite{strain_2023}, and in accordance with previously published guidelines \cite{peer_2021},
participants were required to have completed at least 100 studies on Prolific, and were
required to have a Prolific score of at least 100, indicating acceptance on at least
100/101 previously completed studies. Participants who took part in any of our 
previous studies were prevented from participating, and participants were only
permitted to complete the experiment on a desktop or laptop computer.

Data were collected from 164 participants. 14 failed more than 2 out of 6 attention
checks, and, as per pre-registration stipulations, were rejected from the study. Data
from 150 participants was included in the analysis (`r printnum(gender$M, digits = 0)`% male, `r printnum(gender$F, digits = 0)`% female, and `r printnum(gender$NB, digits = 0)`% non-binary). Mean age of participants was `r printnum(age$mean, digits = 1)` (*SD* = `r printnum(age$sd, digits = 1)`). Mean graph literacy score was `r printnum(literacy$mean)`
(*SD* = `r printnum(literacy$sd)`) out of 30. The average time taken to complete
the experiment was 39 minutes (SD = 14 minutes).

## Stimuli

The data used to generated the scatterplots in the current study were identical to that
used previously \cite{strain_2023}. Scatterplots were generated based on 45 uniformly distributed *r* values between 0.2 and 0.99. Scatterplot points were generated based on bivariate normal 
distributions with standard deviations of 1 in each direction. Each scatterplot
had a 1:1 aspect ratio, was generated as a 1200 x 1200 pixel .png image, and was
scaled up or down according to the participant's monitor. See \autoref{dot-pitch-and-crowdsourced-experiments} for a more detailed discussion of precise point sizes and dot pitch in crowd-sourced experiments.

As in our previous study \cite{strain_2023}, we used equation 1 to map residuals 
to point sizes in three of our conditions. We additionally used a scaling factor of 4 and a constant of 0.2 to achieve a minimum on-screen point size of 12 pixels, which is consistent with the point size on
a 1920 x 1080 monitor for both experiments in \cite{strain_2023}.In our fourth condition, which we refer to as *standard size*, point size was uniformly set to be consistent with the point size in our previous studies. Scripts detailing scatterplot and mask generation can be found in the item
preparation folder in the repository linked below.

\begin{equation}
  point_{size} = 1 - b^{Residual}
\end{equation}

## Dot Pitch and Crowdsourced Experiments

```{r dot-pitch, include = FALSE}
mean_dot_pitch <- mean(exp_size_only_tidy$dot_pitch)

sd_dot_pitch <- sd(exp_size_only_tidy$dot_pitch)
```

In our previous study \cite{strain_2023}, we had no way of obtaining dot pitch
or participant to monitor distance due to the online, crowdsourced nature of the 
experiments. Since then we have adopted a method for obtaining the height of a 
participant's monitor in inches \cite{screenscale}; participants are asked to hold up a standard size
credit/debit/ID card up to the monitor, and then to resize a corresponding image until
it matches the physical size of the card. These cards have a universal
standard size (ISO/IEC 7810 ID-1), which when combined with
the monitor resolution obtained from PsychoPy \cite{pierce_psychopy_2019} 
and assuming a widescreen 16:9 aspect ratio,
allows us to infer dot pitch and therefore the physical size of the points in our
experiment. Mean dot pitch was `r printnum(mean_dot_pitch)`mm, ($SD = `r printnum(sd_dot_pitch)`$),
corresponding to a physical size on the screen of `r printnum(mean_dot_pitch*13)`mm
for the smallest points displayed. See \autoref{results} for analyses including dot pitch as a predictor.

## Visual Threshold Testing

```{r threshold-values, include=FALSE}
vis_df <- exp_size_only_tidy %>%
  group_by(VT_no_correct) %>%
  count()
```

It is key that our manipulation does not remove data from the scatterplot,
thus, in order to test that all our points were visible across a range of viewing
contexts and on a range of apparatus, we included visual threshold testing prior
to the experimental items in the study. Participants were shown six scatterplots
and were asked to enter in a text box how many points
were being displayed. The points were the same size as the smallest points used
in the experimental materials. `r printnum(vis_df$n[1]/270, digits = 0)`% of 
participants were correct on `r printnum(vis_df$VT_no_correct[1])` out of 6 visual
threshold questions, while `r printnum(vis_df$n[2]/270, digits = 0)`% were correct
on `r printnum(vis_df$VT_no_correct[2])` out of 6. It should be noted that those 
participants scoring 5/6 did not answer incorrectly, rather they did not answer
at all for a particular question, which is suggestive of
a mis-click or an initial misunderstanding of the task. Regardless, we consider these results to be indicative of a sufficient level of 
point visibility.

## Design

The experiment used a fully repeated measures, within-participants design, with each
participant seeing and responding to each of the 180 scatterplots in a randomized order.
There were four scatterplots for each of the 45 *r* values corresponding to the
four levels of the size decay condition, examples of which can be see in Figure \ref{fig:examples}.
Everything needed to run the experiment, including code, materials, instructions, and scripts, is
hosted at https://gitlab.pavlovia.org/Strain/exp_size_only.

```{r examples, echo=FALSE, fig.asp=1, fig.show='hold', fig.cap="Four levels of the point size decay condition, demonstrated with an \\textit{r} value of 0.6", out.width="100%", crop = TRUE}
example_plots()
```

## Procedure

Each participant was shown the participant information sheet (PIS) and provided
consent through key presses in response to consent statements. They were asked
to provide their age in a free text box, and their gender identity. Participants
then completed the 5-item Subjective Graph Literacy test \cite{garcia_2016}, 
followed by the visual threshold testing described above. Participants then completed
the screen scaling task described in \autoref{dot-pitch-and-crowdsourced-experiments}.
Participants were given instructions, and then shown examples of *r* = 0.2, 0.5, 0.8, and
0.95. \autoref{training} includes a discussion of the potential effects of viewing these examples. Two practice trials were given before the experiment began. Participants worked through a series of 180 trials
and were asked to use a slider to estimate the correlation shown in
the scatterplot. Visual masks preceded each plot. Interspersed were six attention check trials which asked 
participants to set the slider to 1 or 0 and ignore the scatterplot.

# Results

```{r dot-plot, echo=FALSE, fig.show='hold', fig.cap="Mean error in correlation estimates across the four conditions, with 95\\% confidence intervals. Effect sizes between standard size and other conditions in Cohen's d are also displayed", out.width="100%", crop = TRUE}
dot_plot_function(exp_size_only_tidy) +
  scale_x_discrete(labels = c("Standard\nSize",
                              "Inverted Non-linear\nSize Decay",
                              "Linear Size\nDecay",
                              "Non-linear\nSize Decay")) +
  ylim(0,0.2) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,
                   tip.length = c(0.13,0.11),
               data = data.frame(xmin = 1,
                                 xmax = 4,
                                 label = "d = 0.61",
                                 y.position = 0.2)) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,  
                   tip.length = c(0.17,0.6125),
               data = data.frame(xmin = 2,
                                 xmax = 4,
                                 label = "d = 0.54",
                                 y.position = 0.18)) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,
                   tip.length = c(0.1,0.19),
               data = data.frame(xmin = 3,
                                 xmax = 4,
                                 label = "d = 0.11",
                                 y.position = 0.07))
```

```{r model, cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="size_and_scatterplots/latex/"}
size_model <- buildmer(difference ~ size +
                         (1 + size | participant) +
                         (1 + size | item),
                       data = exp_size_only_tidy)

model <- size_model@model
```

```{r model-comparison,cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="size_and_scatterplots/latex/"}
model_comparison <- comparison(size_model)
```

```{r anova, eval=TRUE, echo=FALSE}
anova_results(size_model, model_comparison)
```

```{r effects-sizes, eval=FALSE, echo=FALSE}
effects_df <- get_effects_sizes(size_model, exp_size_only_tidy)
```

```{r contrasts-table, echo=FALSE}
# outputs summary statistics

contrasts <- contrasts_extract(size_model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  mutate(Contrast = recode(Contrast,
                           "A - C" = "Non-linear Decay : Inverted Non-linear Decay",
                           "A - D" = "Non-linear Decay : Standard Size",
                           "A - B" = "Non-linear Decay: Linear Decay",
                           "B - C" = "Linear Decay : Inverted Non-linear Decay",
                           "B - D" = "Linear Decay : Standard Size",
                           "C - D" = "Inverted Non-linear Decay : Standard Size"))

k <- knitr::kable(contrasts, booktabs = TRUE, digits = c(0,2,3), caption = "Contrasts between the four levels of the size decay condition.", escape = FALSE)

kable_styling(k, latex_options = "scale_down")
```

```{r effects-df, echo=FALSE, include = FALSE}
effects_sizes <- get_effects_sizes(size_model, exp_size_only_tidy) %>%
  rownames_to_column() %>%
  arrange(rowname) %>%
  rename("Comparison" = "rowname",
         "Cohen's d" = "d") %>%
  mutate(Comparison = recode(Comparison,
                          "sizeA" = "Non-linear Size Decay",
                          "sizeB" = "Linear Size Decay",
                          "sizeC" = "Inverted Non-linear Size Decay"))

knitr::kable(effects_sizes, digits = c(0,2), booktabs = TRUE, caption = "The effects sizes of our point size decay conditions when compared to the standard size decay condition.")
```

```{r literacy, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
literacy_model <- lme4::lmer(add.terms(formula(size_model), "literacy"),
                       data = exp_size_only_tidy)

model_literacy_comparison <- size_model
```

```{r literacy-comparison, eval=TRUE,echo=FALSE}
anova_results(model_literacy_comparison, literacy_model)
```

```{r error-plot, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Participants' mean \\textit{r} estimates plotted against the objective \\textit{r} value separately for each size decay condition.", fig.env="figure*", crop = TRUE, out.width= "100%", results=FALSE, fig.pos="h"}
plot_error_bars_function(exp_size_only_tidy, "slider.response", labels_size) +
  xlim(0.2,1)
```

```{r changes-with-r-size, echo=FALSE, fig.align='center', fig.show='hold', fig.cap="Participants' mean errors in \\textit{r} estimates plotted against the objective \\textit{r} value separately for each size decay condition. The dotted horizontal line represents perfectly accurate estimation.", fig.env="figure*", crop = TRUE, out.width= "100%", results=FALSE}
plot_error_bars_function(exp_size_only_tidy, "difference", labels_size) +
    labs(y = "Mean r estimation error") +
    theme(title = element_text(size = 8)) +
    ylim(-0.3, 0.5) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)
```
All analyses were conducted using R (version 4.3.0 \cite{r_core}). Models were
built using the **buildmer** (version 2.8 \cite{voeten_buildmer}) and **lme4**
(version 1.1-32 \cite{bates_lme4_2015}) packages, with size decay condition being set
as the predictor for participants' errors in correlation estimates.

Mean errors in correlation estimates for the four size decay conditions
can be seen in Figure \ref{fig:dot-plot}. A likelihood ratio test revealed that the
model including size decay condition as a predictor explained significantly more
variance than a null model ($\chi^2$(`r in_paren(size_model.df)`) = `r printnum(size_model.Chisq)`,
*p* `r printp(size_model.p, add_equals = TRUE)`). This model has random intercepts for
items and participants. The effect here is driven by participants' errors being lower 
for scatterplots with the non-linear size decay manipulation than for all other conditions,
for error being lower for scatterplots with linear size decay than for plots with
inverted non-linear decay or standard size, and for errors being higher for scatterplots
with standard size than for plots with inverted non-linear decay. 

Testing for contrasts between the four levels of the size decay condition 
was performed with the **emmeans** package (version 1.8.5 \cite{emmeans}), and
are shown in Table \ref{tab:contrasts-table}. The **EMAtools** (version
0.1.4 \cite{ematools}) package was used to calculate effects sizes in Cohen's d, the results of which can be seen in Figure \ref{fig:dot-plot}. The largest effect size we found was 0.61 when comparing
the non-linear size decay and standard size decay conditions. This is significantly higher
than any of the effects sizes we found in our previous work.

In addition, we find no significant difference between the experimental model
and another including graph literacy as a fixed effect ($\chi^2$(`r in_paren(model_literacy_comparison.df)`) = `r printnum(model_literacy_comparison.Chisq)`, *p* `r printp(model_literacy_comparison.p, add_equals = TRUE)`), suggesting the effect we found was not driven by differences in graph literacy.

Figure \ref{fig:changes-with-r-size} shows how participants' mean errors in correlation
estimates change with the objective *r* value, plotted separately for each
size decay condition. Note the close-to-zero average errors present in the non-linear size
decay condition.

```{r dot-pitch-modelling, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
dot_pitch_model <- lme4::lmer(add.terms(formula(size_model), "dot_pitch"),
                              data = exp_size_only_tidy)

model_dot_pitch_comparison <- size_model
```

```{r dot-pitch-comparison, eval=TRUE, echo=FALSE}
anova_results(model_dot_pitch_comparison, dot_pitch_model)
```

We employed a method for obtaining
a measurement of dot pitch from each participant. While \autoref{visual-threshold-testing}
provides evidence that participants had no problems perceiving all the points
shown on the scatterplots, there may be some other facet of using a larger or smaller
monitor with a higher or lower resolution that could have affected the estimates
participants gave. To check this, we built a model including the dot pitch
measurement as a fixed effect. Comparing this to the experimental model revealed
a significant effect of dot pitch ($\chi^2$(`r in_paren(model_dot_pitch_comparison.df)`) = `r printnum(model_dot_pitch_comparison.Chisq)`, *p* `r printp(model_dot_pitch_comparison.p, add_equals = TRUE)`). There was no interaction between size decay condition and dot pitch, with a decrease
in dot pitch of 0.1 resulting in a decrease in estimated correlation of .03. While
significant, we do not consider this effect large enough to warrant further discussion.

# Discussion

As can be seen in Figure \ref{fig:changes-with-r-size},
participants' errors in correlation estimates were significantly lower when 
they were presented with the non-linear size decay condition (see Figure \ref{fig:examples}) 
compared to when they were presented with all other conditions, providing support for our
first hypothesis. We found no support for our second hypothesis, that participants' estimates would be least accurate in the inverted non-linear size decay condition. Errors in this condition were indeed significantly higher than for the other
two size decay conditions, but were significantly lower than
the error with the standard size condition.

The mean error in correlation estimation for the non-linear size decay condition used
in the present study was 0.025, while the equivalent condition in the second experiment
of our previous study, which used the same equations applied to contrast, resulted 
in a mean error of 0.086 \cite{strain_2023}. Taken together, this is evidence that point size is a much stronger channel for the manipulation of perceived correlation in scatterplots than point contrast. If the effects we have found here and in our previous work
are being driven by increased uncertainty in the outer regions of the plots,
that we have found a large effect of point size manipulations is congruent 
with research showing clear influences of stimulus
size on perception and uncertainty \cite{hong_2021, grice_1983, alais_2004}. Contrary to this, the literature linking contrast to perceptual uncertainty is sparse. Unlike our previous work, in which the standard deviations of errors for most conditions became smaller as the objective *r* value increased, participants' 
distributions of standard deviations of correlation estimates remained mostly constant. This
is unexpected, as previous work, including our own, finds precision
in *r* estimation to increase as the objective *r* value increases. Given that we found
this in our work manipulating point contrast \cite{strain_2023}, and 
its robustness in the literature, this result is surprising. We suggest that this
is due to the nature of the stimuli. At high values of *r* there is a large amount
of overlap between points with the non-linear, non-linear inverted, and linear size decay conditions (see examples in the item preparation folder in the repository linked above). It may be that this
is itself producing greater uncertainty and causing an absence of the increased
precision we would expect to see at higher *r* values. While the visual character
of the scatterplots in the aforementioned conditions can account for the absence
of higher precision at higher *r* values, the same cannot be said for the standard size
condition. Aside from the inverted non-linear decay condition in experiment two of
our prior work \cite{strain_2023}, the finding that precision increased with *r*
was robust. Its absence here is curious given that the standard size decay condition in
the present study is identical to the full contrast conditions in our previous work.
Taken together, this suggests that there is something particular about the scatterplots
in the present study that is causing this. Relying on relative judgements means
the interplay between scatterplots with different visual features must be accounted for within a particular experiment. Here, this interplay has resulted in the absence of this effect, albeit
further testing would be required for a more concrete explanation.

The lack of support for our second hypothesis was surprising, although it should be noted that the difference between the inverted non-linear condition and the standard size
condition was small (see Figure \ref{fig:dot-plot}). The shape of the errors
in correlation estimates (see Figure \ref{fig:changes-with-r-size}) is also
very similar to that of the standard size decay condition. It would seem then that
despite the size channel being more powerful than contrast with regards to correcting
for the underestimation of correlation, it is weaker than the contrast channel at
producing the opposing effect. In our previous work we suggested that contrast
manipulations could be used to correct for the *overestimation* of the correlation
of negatively correlated scatterplots; we would not suggest the use of the size
channel for this given the results here.

To conclude, we recommend the use of the non-linear size decay condition described
here when designing scatterplots optimized for correlation perception.

## Training

```{r training-model, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
training_model <- lmer(add.terms(formula(size_model), "half"),
                       data = exp_size_only_tidy)

training_comparison <- size_model
```

```{r training-comparison, eval = TRUE, echo = FALSE, message=FALSE}
anova_results(training_comparison, training_model)
```

Before the experiment, participants viewed plots for a minimum of
eight seconds with examples of *r* = 0.2, 0.5, 0.8, and 0.95. This was to account
for any potential unfamiliarity with scatterplots present in the samples
that we recruited; this risk is inherent in recruiting from lay populations, but we
would argue is acceptable given it leads to more generalisable and broadly
applicable findings. To test whether this training had an effect on correlation estimation,
we built a model including session half as a predictor. Comparing this
to the original model revealed no significant effect ($\chi^2$(`r in_paren(training_comparison.df)`)
= `r printnum(training_comparison.Chisq)`, *p* `r printnum(training_comparison.p, add_equals = TRUE)`),
suggesting that having more recently viewed the example plots did not have an effect
on participants' estimates of correlation.

## Limitations

The data we have gathered is inherently comparative.
Despite confirming a method of obtaining dot pitch, we still have no method of obtaining head-to-monitor distances. Taken together, these aspects of our experiment prevent us from making concrete psychophysical conclusions, but instead allow for findings that are rigorous to different viewing contexts
that we argue are of particular importance for the HCI and design audiences. It
may be that a high level perceptual phenomenon is responsible for the effects
we have seen here; investigating this is beyond the scope of the current study and
does not negate our findings.

## Future Work

At present, we have confirmed the potential for both point contrast and size 
manipulations to influence participants' perceptions of correlation in scatterplots,
each to varying degrees. It is also clear that these manipulations are not necessary,
and may even be making perception worse, at very low and high values of *r*. Our 
future work will therefore take two directions; we will investigate the effect of manipulating
both point size and contrast on correlation estimation, and we will introduce a
parameter to control the strength of this family of manipulations according to the 
objective *r* value itself.

# References {-}





















