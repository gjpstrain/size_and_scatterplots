---
author_1: "Gabriel Strain"
author_2: "Andrew J. Stewart"
author_3: "Paul Warren"
author_4: "Caroline Jay"
author_1_email: "Gabriel.Strain@manchester.ac.uk"
author_2_email: "Andrew.J.Stewart@manchester.ac.uk"
author_3_email: "Paul.Warren@manchester.ac.uk"
author_4_email: "Caroline.Jay@manchester.ac.uk"
affiliation: "The University of Manchester"
acknowledgements: "This work was supported by funding from the University of Manchester Department of Computer Science and Division of Psychology, Communication and Human Neuroscience."
output:
  bookdown::pdf_book: # for automatic figure-numbering (https://bookdown.org/yihui/rmarkdown-cookbook/figure-number.html)
    keep_tex: yes
    template: template.tex
    citation_package: natbib
    extra_dependencies: ["float"]
title: "Adjusting Point Size to Facilitate More Accurate Correlation Perception in Scatterplots"
editor_options: 
  markdown: 
    wrap: 72
bibliography: size-and-scatterplots
abstract: |
  Viewers consistently underestimate correlation in positively correlated scatterplots.
  We use a novel data point size manipulation to correct for this bias. In a high-powered and
  fully reproducible study, we demonstrate that decreasing the size of a point on a scatterplot
  as a function of its distance from the regression line is able to correct for a systematic
  perceptual bias long present in the literature. We recommend the implementation of our technique
  when designing scatterplots that aim to communicate positive correlations.
introduction: |
---

```{r setup, include = FALSE}
knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, crop = TRUE)

#Knitting this document requires tinytex (install.packages("tinytex"))
```

```{r libraries-and-conflicts, include=FALSE}
set.seed(1234) # seed for all random number generation

# Loading packages
library(rticles)
library(tidyverse)
library(MASS)
library(emmeans)
library(scales)
library(buildmer)
library(lme4)
library(kableExtra)
library(afex)
library(papaja)
library(qwraps2)
library(lmerTest)
library(ggdist)
library(ggpubr)
library(conflicted)
library(EMAtools)
library(knitr) #needed for docker container to render paper correctly

# fix conflicts now using the conflicted package

conflicts_prefer(dplyr::select(), dplyr::filter(), lme4::lmer())
```

```{r eval-models, include=FALSE}
# in this script, models are cached. If eval_models <- FALSE, script will load
# cached models. Set eval_models <- TRUE to rebuild models from scratch

eval_models <- FALSE

if (eval_models == FALSE){
  lazyload_cache_dir('size_and_scatterplots/latex')
}
```

```{r load-data, include=FALSE}
# load in data file

size_anon <- read_csv("data/final_data.csv")
```

```{r wrangle, include = FALSE}
# function for wrangling data

## NB: With the exception of anonymisation, data are provided as-is from 
## pavlovia (survey tool). Wrangling function must be run first to make
## the dataset usable

# first do literacy

wrangle <- function(anon_file) {
  
  literacy <- anon_file %>%
    filter(!is.na(q1_slider.response)) %>%
    rowwise() %>%
    mutate(literacy = sum(c(q1_slider.response, 
                            q2_slider.response, 
                            q3_slider.response, 
                            q4_slider.response, 
                            q5_slider.response))) %>%
    select(participant,
           literacy)
  
# extract and process visual threshold testing
  
  visual_thresholds <- anon_file %>%
    filter(!is.na(VT_with_labels)) %>%
    select(c("VT_with_labels", "participant", "VT_textbox2.text")) %>%
    mutate(VT_answer = str_replace(VT_with_labels, pattern = "vis_threshold_plots/", replacement = "")) %>%
    mutate(VT_answer = str_replace(VT_answer, pattern = "_VT.png", replacement = "")) %>%
    mutate(correct_VT = case_when(
      VT_answer == VT_textbox2.text ~ "y",
      VT_answer != VT_textbox2.text ~ "n",
      is.na(VT_answer) ~ "n", TRUE ~ as.character(VT_answer))) %>%
    group_by(participant) %>% 
    summarise(VT_no_correct = sum(correct_VT == "y")) %>%
    select("VT_no_correct", "participant")
  
# extract and process monitor and dot pitch information
# we assume standard 16:9 aspect ratio for monitors
  
  monitor_information <- anon_file %>%
    filter(!is.na(height)) %>%
    filter(!is.na(res_width)) %>%
    mutate(res_height = res_width*0.5625,
           width = height*0.5625,
           dot_pitch = ((sqrt(height^2 + width^2))/(sqrt(res_height^2 + res_width^2))) * 25.4) %>%
        select(c("dot_pitch", "participant", "res_width"))
    
  
# extract demographic information
# link slider response numbers to gender categories
  
  demographics <- anon_file %>%
    filter(!is.na(gender_slider.response)) %>%
    mutate(gender_slider.response = recode(gender_slider.response,
                                         `1` = "F",
                                         `2` = "M",
                                         `3` = "NB")) %>%
  select(matches(c("participant",
                          "age_textbox.text",
                          "gender_slider.response")))

# split images column into item and condition columns 

anon_file <- anon_file %>%
  mutate(images = str_replace(images, pattern = "A", replacement = "-A")) %>%
  mutate(images = str_replace(images, pattern = "B", replacement = "-B")) %>%
  mutate(images = str_replace(images, pattern = "C", replacement = "-C")) %>%
  mutate(images = str_replace(images, pattern = "D", replacement = "-D")) %>%
  separate(images, c("item", "size"), sep = "-") %>%
  mutate(size = str_replace(size, pattern = ".png", replacement = "")) %>%
  mutate(item = str_replace(item, pattern = "all_plots/", replacement = ""))

# select relevant columns
# select only experimental items
# add literacy data
# change data types where appropriate
# output this file with suffix 'tidy'

anon_file %>%
  select(c("participant",
                  "item",
                  "size",
                  "slider.response",
                  "my_rs",
                  "total_residuals",
                  "unique_item_no",
                  "session",
                  "trials.thisN")) %>%
  mutate(half = case_when(
    trials.thisN < 93 ~ "First",
    trials.thisN > 92 ~ "Second" )) %>%
  filter(unique_item_no < 181) %>%
  inner_join(literacy, by = "participant") %>%
  inner_join(demographics, by = "participant") %>%
  inner_join(monitor_information, by = "participant") %>%
  inner_join(visual_thresholds, by = "participant") %>%
  mutate(across(matches(c("item", "size")), as_factor)) %>%
  select(-c("__participant")) %>%
  mutate(difference = my_rs - slider.response) %>%
  mutate(size = fct_relevel(size, c('D', 'C', 'B', 'A'))) %>% 
  assign(paste0(unique(anon_file$expName), "_tidy"),
           value = ., envir = .GlobalEnv)
}

# use wrangle function on anonmyised data file 

wrangle(size_anon)

# remove anon df from environment

rm(size_anon)

# extract age data

age <- distinct(exp_size_only_tidy, participant,
                .keep_all = TRUE) %>%
  summarise(mean = mean(age_textbox.text, na.rm = TRUE),
            sd = sd(age_textbox.text, na.rm = TRUE)) 

# extract gender data

gender <- distinct(exp_size_only_tidy, participant,
                      .keep_all = TRUE) %>%
  group_by(gender_slider.response) %>%
  summarise(perc = n()/nrow(.)*100) %>%
  pivot_wider(names_from = gender_slider.response, values_from = perc)

# extract literacy data

literacy <- distinct(exp_size_only_tidy, participant,
                        .keep_all = TRUE) %>%
  summarise(mean = mean(literacy), sd = sd(literacy))
```

```{r comparison-function, include=FALSE}
# this function takes a model and creates a nested model with the fixed effects 
# term removed for anova comparison

comparison <- function(model) {
  
  parens <- function(x) paste0("(",x,")")
  onlyBars <- function(form) reformulate(sapply(findbars(form),
                                              function(x)  parens(deparse(x))),
                                       response=".")
  onlyBars(formula(model))
  cmpr_model <- update(model,onlyBars(formula(model)))
  
  return(cmpr_model)
  
}
```

```{r anova-results-function, include=FALSE}
# this function takes two nested models, runs an anova, and the outputs the 
# Chi-square statistic, the degrees of freedom, and the p value to the global environment

anova_results <- function(model, cmpr_model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  if (class(cmpr_model) == "buildmer") cmpr_model <- cmpr_model@model
  
  anova_output <- anova(model, cmpr_model)
  
  assign(paste0(model_name, ".Chisq"),
         anova_output$Chisq[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".df"),
         anova_output$Df[2],
         envir = .GlobalEnv)
  assign(paste0(model_name, ".p"),
         anova_output$`Pr(>Chisq)`[2],
         envir = .GlobalEnv)
  
}
```

```{r contrasts-extract, echo=FALSE}
# this function extracts test statistics and p values from model summaries

contrasts_extract <- function(model) {
  
  model_name <- deparse(substitute(model))
  
  if (class(model) == "buildmer") model <- model@model
  
  EMMs <- emmeans(model, pairwise ~ size)
  
  params <- as.data.frame(EMMs[2]) %>%
                            rename_with(str_replace,
                                        pattern = "contrasts.", replacement = "",
                                        matches("contrasts")) %>%
                            rename_with(str_to_title, !starts_with("p")) %>%
                            select(c("Contrast", "Z.ratio", "p.value"))
  
  return(params)
  
}
```

```{r effect-size-function, include = FALSE}
# function to calculate effect sizes in Cohen's d from models

get_effect_sizes <- function(model, d) {
  
  effect_sizes <- lme.dscore(model, data = d, type = "lme4")
  
  effects_df <- as.data.frame(effect_sizes[3])
  
  return(effects_df)
}
```

```{r dot-plot-function, include = FALSE}
# function to create dot plots showing average correlation estimation errors and 95% CIs

dot_plot_function <- function(df) {

data <- df %>%
  group_by(size) %>%
  filter(!is.na(difference)) %>%
  filter(!is.na(size)) %>%
  summarise(
    mean = mean(difference),
    lci = t.test(difference, conf.level = 0.95)$conf.int[1],
    hci = t.test(difference, conf.level = 0.95)$conf.int[2],
  )

  data %>%
    mutate(size = fct_relevel(size, "D", "C", "B", "A")) %>%
    ggplot(aes(x = size, y = mean)) +
    geom_point(stat = "identity", size = 1) +
    geom_errorbar(aes(ymin=lci, ymax=hci), colour ="black", width=0.01, linewidth =0.5) +
    theme_ggdist() +
    labs(x = "Point Size Condition",
         y = "Mean Error") +
    theme(axis.text = element_text(size = 13),
          axis.title = element_text(size = 16))
}
```

```{r error-bar-plot, include = FALSE}
# plot the error bars plots by condition

plot_error_bars_function <- function(df, measure, l){
  
  df %>% 
  drop_na() %>%
  group_by(size, my_rs) %>% 
  summarise(sd = sd(get(measure)), mean = mean(get(measure))) %>% 
  ggplot(aes(x = my_rs, y = mean)) +
  #geom_point(size = 0.2) + 
  geom_errorbar(mapping = aes(ymin = mean + sd, ymax = mean - sd, alpha = 0.4),width = 0.01, size = 0.3) +
  theme_ggdist() +
  scale_y_continuous(breaks = seq(0,1, 0.2)) +
  theme(strip.text = element_text(size = 6, margin = margin(1,0,1,0, "mm")), aspect.ratio = 1,
        axis.text = element_text(size = 6.5),
        axis.title = element_text(size = 8),
        legend.position = "none") +
  facet_wrap(size ~., ncol = 4, labeller = labeller(size = l)) +
    labs(x = "Objective r",
         y = "Mean r estimation") +
    geom_line(formula= x ~ y) +
    xlim(0.2,1)
}

```


```{r labeller, include = FALSE}
# creates labels vector for use with plotting functions

labels_size <- c(A = "Non-Linear Decay (.025)", B = "Linear Decay (.041)", C = "Inverted Decay (.140)", D = "Standard Size (.167)")
```

```{r example-plots, include = FALSE}
# creates stimuli example plots with r = 0.6

example_plots <- function () {
  
  set.seed(1234)
  
  my_sample_size = 128
  
  my_desired_r = 0.6
  
  mean_variable_1 = 0
  sd_variable_1 = 1
  
  mean_variable_2 = 0
  sd_variable_2 = 1
  
  mu <- c(mean_variable_1, mean_variable_2) 
  
  myr <- my_desired_r * sqrt(sd_variable_1) * sqrt(sd_variable_2)
  
  mysigma <- matrix(c(sd_variable_1, myr, myr, sd_variable_2), 2, 2) 
  
  corr_data = as_tibble(mvrnorm(my_sample_size, mu, mysigma, empirical = TRUE))
  
  corr_model <- lm(V2 ~ V1, data = corr_data)
  
  my_residuals <- abs(residuals(corr_model))
  
  data_with_resid <- round(cbind(corr_data, my_residuals), 2)
  
slopes <- data_with_resid %>%
  mutate(slope_linear = my_residuals/3.2) %>%
  mutate(slope_0.25 = 1-(0.25)^my_residuals) %>%
  mutate(slope_inverted = (1 + (0.25)^ my_residuals)-1)
  
plot_example_function <- function (d, x, t) {
  
set.seed(1234)
  
  ggplot(d, aes(x = V1, y = V2)) +
  scale_size_identity() +
  geom_point(aes(size = 4*(x + 0.2)), shape = 16)  +
  labs(x = "", y = "") +
  theme_classic() +
  theme(axis.text = element_blank(),
        plot.margin = unit(c(0,0,0,0), "cm"),
        legend.position = "none",
        plot.title = element_text(size = 15)) +
  labs(title = t)

}  

plots <- ggarrange(plot_example_function(slopes, (1-slopes$slope_0.25), "Non-linear Decay"),
                   plot_example_function(slopes, (1-slopes$slope_linear), "Linear Decay"),
                   plot_example_function(slopes, (1-slopes$slope_inverted), "Inverted Non-linear Decay"),
                   plot_example_function(slopes, 0.05, "Standard Size"))

return(plots)

}
```

# Introduction

Scatterplots, utilized in scientific communication for a variety of tasks,
are some of the most widely used and studied data visualizations. Viewers
interpret them in similar ways \cite{kay_heer_2015}, and they are simple
to study while providing important insights into visualization
design, human-computer interaction, and perception. Previously \cite{strain_2023}
we showed that a novel point contrast manipulation, in which the contrast of a certain
scatterplot point was reduced as the size of that point's residual increased, could be
used to partially correct for a systematic correlation underestimation bias present in the 
literature \cite{strahan_1978, bobko_1979, cleveland_1982, lane_1985, lauer_1989, 
collyer_1990, meyer_1992}. We suggested that this was due to a narrowing of the width
of the perceived probability distribution of the data in a plot relative to the regression line,
presumably driven by a reduction in the salience of, or weight given to, the lower
contrast points in those outer areas. We tested linear, non-linear, 
and inverted non-linear decay functions relating point contrast to
residual magnitude. As anticipated, we found that the non-linear function produced
the most accurate estimates of correlation, and that the non-linear inverted produced 
the least accurate.

In this paper we utilize the same functions applied to point size to demonstrate that 
a non-linear decay function can be employed to correct for a systematic underestimation of correlation.
We find no evidence for effects of graph literacy or training.
The effect we observe here is stronger, both with regards to effect size
and in terms of the observed reduction in error, than that
observed in our previous study \cite{strain_2023}. We suggest that this 
approach can be used to facilitate more accurate correlation
perception in scatterplots while providing exciting future avenues for the continuation
and refinement of these techniques.

## Scatterplots and Correlation

Scatterplots have been widely studied, especially as mediums for the communication
of correlation. Our previous work contains a review of the history of
this literature \cite{strain_2023}. Previous studies have found evidence for a 
pronounced underestimation in judgements of correlation in positively correlated
scatterplots, especially between 0.2 < *r* < 0.6. This
is true for both direct estimation \cite{meyer_1992, collyer_1990} and estimation
via bisection tasks \cite{rensink_2017}. Additionally, methods from psychophysics
have been employed to explore how we discriminate between different correlations
\cite{rensink_2014, rensink_2017}. Presently, we use the direct estimation paradigm 
owing to its simplicity and its suitability to online experimentation. The judgements we collect are therefore
comparative by nature, as we analyse the differences in correlation estimation performance
between data-identical scatterplots with different point size manipulations (see Figure \ref{fig:examples}).
Such work allows us to inform design guidelines and affords us insights into perception.
It is our duty as visualization designers to ensure that the
messages we are trying to communicate are being interpreted accurately.
To achieve this, we must understand human perception, apply that understanding
to design, and test those designs in rigorous empirical studies.

## Point Size

Contrast adjustments have been used extensively to solve issues of overplotting and clutter
in scatterplots \cite{matejka_2015, bertini_2004}. Practicality dictates that 
scatterplots visualizing large datasets inherently require their points to be
smaller to prevent obfuscation of the data. Point size changes (e.g bubble charts)
have been used to represent trivariate data. There has, however, been little testing of the 
impact of point size on correlation perception. Studies have found 
invariance in the bias and variability of correlation 
perception with regards to changing point sizes, but these have been low-powered
\cite{rensink_2012, rensink_2014}. Our motivation is to extend our previous work
\cite{strain_2023} to further improve correlation perception in scatterplots, while
providing visualization designers with tools for more effective design.
From the wider literature there is evidence that larger points can bias judgements of 
mean point position more strongly than point contrast \cite{hong_2021} and can 
result in faster reaction times to peripherally presented stimuli \cite{grice_1983}.
There is some evidence that larger stimuli are associated with lower levels of
spatial certainty \cite{alais_2004} but higher levels of salience \cite{healey_2012}.
The current experiment should therefore allow us to distinguish between these
candidate drivers of the effects observed in a way that was not possible
when manipulating contrast, as in that case effects of salience or spatial
certainty would operate in the same direction.

## Hypotheses

We hypothesized that correlation estimates would be most accurate when
viewers are presented with the non-linear size decay condition, and would be 
least accurate when presented with the non-linear inverted size decay condition.

# Methodology

The experiment was conducted according to the principles of open and reproducible research.
Data and analysis code are available at https://github.com/gjpstrain/size_and_scatterplots.
This repository contains instructions for building a docker image to 
reproduce the computational environment used, allowing for full replications
of stimulus generation, analyses, and the paper itself. Ethical approval was granted by the University
of Manchester's Computer Science Departmental Panel (Ref: 2022-14660-24397).
Hypotheses and analysis plans were pre-registered with the OSF (https://osf.io/k4gd8).

## Stimuli

45 uniformly distributed *r* values were generated between 0.2 and 0.99. 
For each of these values, a scatterplot consisting of a series of 128 data points
was generated based on a normal distribution. The random seed used was the same
as that used in our previous work \cite{strain_2023}, meaning the datasets were
identical. This range of *r* values was chosen as there is evidence 
that little correlation is perceived below *r* = 0.2 \cite{strahan_1978, bobko_1979, cleveland_1982}.
A total of 45 *r* values were chosen so that we could build a more detailed picture
of people's perceptions of correlations than previous work using fewer values,
in addition to affording us a more objective method of comparing people's judgements
beyond semantic labels such as 'weak' or 'strong' correlation as response measures.
We use only positive *r* values, building on previous work that has done the same.
Each scatterplot had a 1:1 aspect ratio, was generated as a 1000 x 1000 pixel .png image, and was
scaled up or down according to the participant's monitor. See
\autoref{dot-pitch-and-crowdsourced-experiments} for a more detailed discussion of 
precise point sizes and dot pitch in crowd-sourced experiments.

We used equation 1 to map residuals 
to point sizes in the two non-linear decay conditions. 0.25 was chosen as the 
value for *b* due to both its prior use with contrast decay \cite{strain_2023} and its
production of a curve approximating the inverse arodun the identity line of the
underestimation curve reported in previous literature \cite{rensink_2017}.
We acknowledge that more suitable values of *b* may exist, but extensive 
testing of this is outside the scope of the current study.
We adjusted residual values using a scaling factor of 4 and a
constant of 0.2 to achieve a minimum on-screen point size of 12 pixels, which 
is consistent with the point size on a 1920 x 1080 pixel monitor in our previous study
\cite{strain_2023}. In our fourth condition,
which we refer to as *standard size*, point size was uniformly
set to be consistent with that in our previous 
work. Scripts detailing scatterplot and mask generation are in the item
preparation folder in the repository linked above.

\begin{equation}
  point_{size} = 1 - b^{residual}
\end{equation}

## Dot Pitch and Crowdsourced Experiments

```{r dot-pitch, include = FALSE}
# extract mean and SD of dot pitch

mean_dot_pitch <- mean(exp_size_only_tidy$dot_pitch)

sd_dot_pitch <- sd(exp_size_only_tidy$dot_pitch)

range_dot_pitch <- range(exp_size_only_tidy$dot_pitch)
```

Previously \cite{strain_2023}, we had no way of obtaining dot pitch
or participant-to-monitor distance due to the online, crowdsourced nature of the 
experiments. We have since adopted a method for obtaining the height of a 
participant's monitor in inches \cite{screenscale}; participants are asked to hold up a standard size
credit/debit/ID card up to the monitor, and then to resize a corresponding image until
it matches the physical size of the card. These cards have a universal
standard size (ISO/IEC 7810 ID-1), which when combined with
the monitor resolution obtained from PsychoPy \cite{pierce_psychopy_2019} 
and assuming a widescreen 16:9 aspect ratio,
allows us to infer dot pitch and therefore the physical size of the points in our
experiment. Mean dot pitch was `r printnum(mean_dot_pitch)`mm ($SD = `r printnum(sd_dot_pitch)`$),
corresponding to a physical size on the screen of `r printnum(mean_dot_pitch*13)`mm
for the smallest points displayed. See \autoref{results} for analyses including dot pitch as a predictor.

## Point Visibility Testing

```{r threshold-values, include=FALSE}
# extract percentage correct on visual thresholds

vis_df <- exp_size_only_tidy %>%
  group_by(VT_no_correct) %>%
  count()
```

It is key that our manipulation does not remove data from the scatterplot,
thus, we include point visibility testing prior to the experimental items in the study.
Participants were shown six scatterplots
and were asked to enter in a text box how many points
were being displayed. The points were the same size as the smallest points used
in the experimental materials. `r printnum(vis_df$n[1]/270, digits = 0)`% of 
participants were correct on `r printnum(vis_df$VT_no_correct[1])` out of 6 point
visibility questions, while `r printnum(vis_df$n[2]/270, digits = 0)`% were correct
on `r printnum(vis_df$VT_no_correct[2])` out of 6. It should be noted that those 
participants scoring 5/6 did not answer incorrectly, rather they did not answer
at all for a particular question, which is suggestive of
a mis-click or an initial misunderstanding of the task. Regardless, 
we consider these results to be indicative of a sufficient level of 
point visibility.

## Design

The experiment used a fully repeated measures, within-participants design, with each
participant seeing and responding to each of the 180 scatterplots in a randomized order.
There were four scatterplots for each of the 45 *r* values corresponding to the
four levels of the size decay condition, examples of which can be see in Figure \ref{fig:examples}.
Everything needed to run the experiment, including code, materials, instructions, and scripts, is
hosted at https://gitlab.pavlovia.org/Strain/exp_size_only.

```{r examples, echo=FALSE, fig.asp=1, fig.show='hold', fig.cap="Four levels of the point size decay condition, demonstrated with an \\textit{r} value of 0.6", out.width="100%"}
# plot examples

example_plots()
```

## Procedure

Each participant was shown the participant information sheet and provided
consent through key presses in response to consent statements. They were asked
to provide their age in a free text box, and their gender identity. Participants
then completed the 5-item Subjective Graph Literacy test \cite{garcia_2016}, 
followed by the point visibility test described above and the screen scaling task.
Participants were given instructions, and then shown examples of *r* = 0.2, 0.5, 0.8, and
0.95. \autoref{training} includes a discussion of the potential training effects of
viewing these examples. Two practice trials were given before the experiment began.
Participants worked through a series of 180 trials
and were asked to use a slider to estimate the correlation shown in
the scatterplot to 2 decimal places. Visual masks preceded each plot. Interspersed were six attention
check trials which explicitly asked participants to set the slider to 1 or 0 and ignore the scatterplot.

## Participants

150 participants were recruited using the Prolific.co platform. Normal to
corrected-to-normal vision and English fluency were required for participation. In
accordance with published guidelines \cite{peer_2021},
participants were required to have completed at least 100 studies on Prolific, and were
required to have a Prolific score of at least 100, indicating acceptance on at least
100/101 previously completed studies. Participants who took part in any of our 
previous studies on correlation perception in scatterplots
were prevented from participating, and participants were only
permitted to complete the experiment on a desktop or laptop computer.

Data were collected from 164 participants. 14 failed more than 2 out of 6 attention
checks, and, as per pre-registration stipulations, were rejected from the study. Data
from 150 participants was included in the analysis (`r printnum(gender$M, digits = 0)`%
male, `r printnum(gender$F, digits = 0)`% female, and `r printnum(gender$NB, digits = 0)`%
non-binary). Mean age of participants was `r printnum(age$mean, digits = 1)`
(*SD* = `r printnum(age$sd, digits = 1)`). Mean graph literacy score was `r printnum(literacy$mean)`
(*SD* = `r printnum(literacy$sd)`) out of 30. The average time taken to complete
the experiment was 39 minutes (*SD* = 14 minutes).

# Results

```{r dot-plot, echo=FALSE, fig.show='hold', fig.cap="Mean error in correlation estimates across the four conditions, with 95\\% confidence intervals. Effect sizes between standard size and other conditions in Cohen's d are also displayed.", out.width="100%"}
# average errors in r estimation for the four experimental conditions
# use geom_bracket() to add brackets for effect sizes between conditions

dot_plot_function(exp_size_only_tidy) +
  scale_x_discrete(labels = c("Standard\nSize",
                              "Inverted Non-linear\nSize Decay",
                              "Linear Size\nDecay",
                              "Non-linear\nSize Decay")) +
  ylim(0,0.2) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,
                   tip.length = c(0.13,0.11),
               data = data.frame(xmin = 1,
                                 xmax = 4,
                                 label = "d = 0.61",
                                 y.position = 0.2)) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,  
                   tip.length = c(0.17,0.6125),
               data = data.frame(xmin = 2,
                                 xmax = 4,
                                 label = "d = 0.54",
                                 y.position = 0.18)) +
  geom_bracket(aes(xmin = xmin,
                   xmax = xmax,
                   label = label),
                   alpha = 0.5,
                   tip.length = c(0.1,0.19),
               data = data.frame(xmin = 3,
                                 xmax = 4,
                                 label = "d = 0.11",
                                 y.position = 0.07))
```

```{r model, cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="size_and_scatterplots/latex/"}
# build model with error being predicted by size condition

size_model <- buildmer(difference ~ size +
                         (1 + size | participant) +
                         (1 + size | item),
                       data = exp_size_only_tidy)

# assign this to 'model' object
# makes it an lmer object instead of buildmer

size_model <- size_model@model
```

```{r model-comparison,cache=eval_models, cache.comments=FALSE, eval=eval_models, include=FALSE, cache.path="size_and_scatterplots/latex/"}
# use comparison function to create null model

model_comparison <- comparison(size_model)
```

```{r anova, eval=TRUE, echo=FALSE}
# run anova on exp and null models

anova_results(size_model, model_comparison)
```

```{r effect-sizes, eval=FALSE, echo=FALSE}
effects_df <- get_effect_sizes(size_model, exp_size_only_tidy)
```

```{r contrasts-table, echo=FALSE}
# outputs summary statistics

contrasts <- contrasts_extract(size_model) %>%
  mutate(p.value = scales::pvalue(p.value)) %>%
  mutate(Contrast = recode(Contrast,
                           "D - C" = "Standard Size : Inverted Non-linear Decay",
                           "D - B" = "Standard Size : Linear Decay",
                           "D - A" = "Standard Size : Non-Linear Decay",
                           "C - B" = "Inverted Non-linear Decay : Linear Decay",
                           "C - A" = "Inverted Non-linear Decay : Non-Linear Decay",
                           "B - A" = "Linear Decay : Non-Linear Decay"))

# create df for table

contrasts_table <- knitr::kable(contrasts, booktabs = TRUE, digits = c(0,2,3), caption = "Contrasts between the four levels of the size decay condition.", escape = FALSE)

# produce table

kable_styling(contrasts_table, latex_options = "scale_down")
```

```{r effects-df, echo=FALSE, include = FALSE}
# create table for effect sizes
# not included in final manuscript

effect_sizes <- get_effect_sizes(size_model, exp_size_only_tidy) %>%
  rownames_to_column() %>%
  arrange(rowname) %>%
  rename("Comparison" = "rowname",
         "Cohen's d" = "d") %>%
  mutate(Comparison = recode(Comparison,
                          "sizeA" = "Non-linear Size Decay",
                          "sizeB" = "Linear Size Decay",
                          "sizeC" = "Inverted Non-linear Size Decay"))

knitr::kable(effect_sizes, digits = c(0,2), booktabs = TRUE, caption = "The effect sizes of the point size decay conditions when compared to the standard size condition.")
```

```{r literacy, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
# build model including graph literacy as fixed effect

literacy_model <- lmer(add.terms(formula(size_model), "literacy"),
                       data = exp_size_only_tidy)

# build null model for graph literacy

model_literacy_comparison <- size_model
```

```{r literacy-comparison, eval=TRUE,echo=FALSE}
# anova on graph literacy models

anova_results(model_literacy_comparison, literacy_model)
```

```{r error-plot, eval=FALSE, fig.align='center', fig.cap="Participants' mean \\textit{r} estimates plotted against the objective \\textit{r} value separately for each size decay condition.", fig.env="figure*", fig.pos="h", fig.show='hold', include=FALSE, out.width="100%", results=FALSE}
# plots r estimates against objective r value
# not included in final manuscript 

plot_error_bars_function(exp_size_only_tidy, "slider.response", labels_size) +
  xlim(0.2,1)
```

```{r differences-plot, echo=FALSE, fig.show='hold', fig.cap="Participants' mean errors in \\textit{r} estimates plotted against the objective \\textit{r} value for each size decay condition. Error bars represent standard deviations. The dotted horizontal line represents accurate estimation. Means of errors by condition are also displayed next to plot titles.", fig.env="figure*", out.width= "100%"}
# plots r estimation errors against objective r value, separately for each condition

plot_error_bars_function(exp_size_only_tidy, "difference", labels_size) +
    labs(y = "Mean r estimation error") +
    theme(title = element_text(size = 8)) +
    ylim(-0.3, 0.5) +
    xlim(0.2,1) +
    geom_hline(yintercept = 0, linetype = 2)
```

All analyses were conducted using R (version 4.3.1 \cite{r_core}). Linear mixed effects models were
built using the **buildmer** (version 2.9 \cite{voeten_buildmer}) and **lme4**
(version 1.1-34 \cite{bates_lme4_2015}) packages, with size decay condition being set
as the predictor for participants' errors in correlation estimates. The experimental
model has random intercepts for items and participants.

Mean errors in correlation estimates and 95% confidence intervals
for the four size decay conditions can be seen in Figure \ref{fig:dot-plot}.
A likelihood ratio test revealed that the model including size decay condition 
as a predictor explained significantly more variance than a null model
($\chi^2$(`r in_paren(size_model.df)`) = `r printnum(size_model.Chisq)`,
*p* `r printp(size_model.p, add_equals = TRUE)`).  The effect here is driven by
significant differences in correlation estimation error between all levels 
of size decay condition. Figure \ref{fig:differences-plot} shows how participants' mean errors in correlation
estimates change with the objective *r* value, plotted separately for each
size decay condition. Overall condition means are also shown in the plot titles.
Note the close-to-zero average errors present in the non-linear size decay condition.  

Testing for contrasts between was performed with the **emmeans** package
(version 1.8.8 \cite{emmeans}), and are shown in Table \ref{tab:contrasts-table}. 
The **EMAtools** (version 0.1.4 \cite{ematools}) package was used to calculate effect sizes in Cohen's d,
the results of which can be seen in Figure \ref{fig:dot-plot}. 
The largest effect size we found was 0.61 when comparing
the non-linear size decay and standard size decay conditions, compared to
0.19 for the equivalent comparison in our previous work \cite{strain_2023}.

We find no significant difference between the experimental model
and one including graph literacy as a fixed effect 
($\chi^2$(`r in_paren(model_literacy_comparison.df)`) = 
`r printnum(model_literacy_comparison.Chisq)`, *p* 
`r printp(model_literacy_comparison.p, add_equals = TRUE)`), 
suggesting the effect we found was not driven by differences in graph literacy.

```{r dot-pitch-modelling, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
# build model with dot pitch included as fixed effect

dot_pitch_model <- lmer(add.terms(formula(size_model), "dot_pitch"),
                              data = exp_size_only_tidy)

# build dot pitch null model
# just use the standard model without dot pitch as fixed effect

model_dot_pitch_comparison <- size_model
```

```{r dot-pitch-comparison, eval=TRUE, echo=FALSE}
# anova on dot pitch modelling

anova_results(model_dot_pitch_comparison, dot_pitch_model)
```

We employed a method for obtaining a measurement of dot pitch from each participant.
While participants performed well on a point visibility task,
there may be some other facet of using a larger or smaller
monitor with a higher or lower resolution that could have affected the estimates
participants gave. To check this, we built a model including dot pitch as a fixed effect. 
Comparing this to the experimental model revealed
a significant effect of dot pitch ($\chi^2$(`r in_paren(model_dot_pitch_comparison.df)`) 
= `r printnum(model_dot_pitch_comparison.Chisq)`, *p* `r printp(model_dot_pitch_comparison.p, add_equals = TRUE)`).
There was no interaction between size decay condition and dot pitch, with a decrease in dot
pitch of 0.1 resulting in a decrease in estimated correlation of .03. Given
that dot pitch range was only `r printnum(range_dot_pitch[1])` to 
`r printnum(range_dot_pitch[2])`, we do not consider this effect substantial 
enough to warrant further discussion.

# Discussion

As can be seen in Figure \ref{fig:differences-plot},
participants' errors in correlation estimates were significantly lower in the
the non-linear size decay condition (see Figure \ref{fig:examples}) 
compared to all other conditions, providing support for our
first hypothesis. We found no support for our second hypothesis, that participants'
estimates would be least accurate in the inverted non-linear size decay condition.
Errors in this condition were significantly higher than for the other two 
size decay conditions, but were significantly lower than
the error with the standard size condition.

## Increased Correlation Estimation Accuracy

The mean signed error in correlation estimation for the non-linear size decay condition used
in the present study was .025, while the equivalent condition in the second experiment
of our previous study, which used the same equations applied to contrast, resulted 
in a mean error of .086 \cite{strain_2023}. This provides evidence that point
size is a stronger encoding channel for the manipulation of perceived correlation in
scatterplots than point contrast. If these effects are driven by the lower salience of
points in the outer regions of the plots,
that we have found a larger effect of point size manipulations is congruent 
with research showing clear influences of stimulus size on object salience and 
perceptual weighting \cite{grice_1983, healey_2012, hong_2021}. Our results
therefore provide support for point salience/weight being the key driver
the effects observed. Lower point salience in the outer regions of the plots
then reduces the perceived width of the distribution of data points about
the regression line \cite{meyer_1997, rensink_2017}, leading in this case to more
more accurate estimates. Simultaneously we acknowledge
that other candidate mechanisms exist; similar results would be expected if a 
feature-based attentional bias was at play \cite{hong_2021, sun_2016}. 
It may be that both attention and perceived probability distribution width 
are responsible for the effects we see, however with our current methodology 
we are unable to comment as to the extent of each.

The lack of support for our second hypothesis, while surprising, suggests that 
point salience or perceived distribution width do not form the whole story. There is
evidence that larger stimuli exhibit higher spatial uncertainty \cite{alais_2004},
and it is possible that this uncertainty causes the perceptual system to downweight
the contribution of these points during correlation estimation. This is consistent
with previous \cite{warren_2002, warren_2004} work suggesting the brain may make
robust statistical use of visuo-spatial information. These mechanisms act to
downweight the influence of more unreliable information (in this case the higher
spatial uncertainty of larger exterior points) on subsequent perceptual estimates. 
Despite the size channel being
more powerful with regards to correcting for correlation underestimation, it appears 
unable to produce the opposing effect. In our previous work we suggested that inverted contrast
manipulations could be used to correct for the *overestimation* of the correlation
of negatively correlated scatterplots; in light of our findings we would not 
suggest the use of the size channel for this given the results here.

## Precision in Correlation Estimation is Constant

Unlike our previous work, in which the standard deviations of errors generally 
became smaller as the objective *r* value increased, distributions of standard 
deviations of correlation estimates here remained mostly constant. This
is unexpected, as previous work, including our own, finds precision
in *r* estimation to increase as the objective *r* value increases. Given that we found
this in our work manipulating point contrast \cite{strain_2023}, and 
its robustness in the literature, this result is surprising. We suggest that this
is due to the nature of the stimuli. At high values of *r* there is a large amount
of overlap between points in the non-linear, non-linear inverted,
and linear size decay conditions. It may be that this
is producing greater uncertainty and causing an absence of the increased
precision we would expect. While the visual character
of the scatterplots in the aforementioned conditions can account for the absence
of higher precision at higher *r* values, this cannot be said for the standard size
condition. Aside from the inverted non-linear decay condition in
our previous work \cite{strain_2023}, the finding that precision increased with *r*
was robust. Its absence here is curious given that the standard size decay condition
here is identical to the full contrast conditions in our previous work.
Relying on relative judgements means the interplay between scatterplots with 
different visual features must be accounted for within a particular experiment.
The stimuli as *r* approaches 1 in the current study exhibit greater levels of
visual variance than the stimuli in our previous work \cite{strain_2023},
which may explain the lack of increased precision here. Further testing is required
for a more concrete explanation.

Ultimately we aim to provide tools for the design of visualizations more suited for the tasks
they are intended to support. When that task is the perception of positive correlation, we would
recommend the use of the non-linear size decay condition described here. We acknowledge that 
for other scatterplot tasks, such as cluster separation or numerosity perception, or
other chart types, the use of the size manipulation may in fact be a hindrance, and in scenarios
where the intended usage of a scatterplot includes tasks such as these, we would not
recommend it. Instead we may recommend the use of a contrast decay condition
\cite{strain_2023}, which also corrects for the underestimation bias (albeit to a
lesser degree), while not impeding other tasks.

## Training

```{r training-model, cache=eval_models, cache.comments=FALSE, eval=eval_models, echo=FALSE, dependson="model", cache.path="size_and_scatterplots/latex/"}
# build model including session half as fixed effect

training_model <- lmer(add.terms(formula(size_model), "half"),
                       data = exp_size_only_tidy)

# build session half null model
# just use the standard model without session half as fixed effect

training_comparison <- size_model
```

```{r training-comparison, eval = TRUE, echo = FALSE, message=FALSE}
# anova on session half model

anova_results(training_comparison, training_model)
```

Before the experiment, participants viewed plots for a minimum of
eight seconds with examples of *r* = 0.2, 0.5, 0.8, and 0.95. This was to account
for any unfamiliarity with scatterplots present in the samples
that we recruited; this risk is inherent in recruiting from lay populations, but we
would argue is acceptable given it leads to more generalizable and broadly
applicable findings. Comparing a model including session half as a predictor with the
original model revealed no significant effect ($\chi^2$(`r in_paren(training_comparison.df)`)
= `r printnum(training_comparison.Chisq)`, *p* `r printnum(training_comparison.p, add_equals = TRUE)`),
suggesting that having more recently viewed the example plots did not have an effect
on participants' performance.

## Limitations

Despite confirming a method of obtaining dot pitch, we still have no method of obtaining
head-to-monitor distances. This, along with the comparative judgements we collect, prevents us 
from drawing concrete psychophysical conclusions. Instead, our paradigm allows
for findings that are rigorous to different viewing contexts and
are of particular importance for the HCI and visualization design audiences. It
may be that a high level perceptual phenomenon is responsible for the effects
we have seen here; investigating this is beyond the scope of the current study and
does not negate our findings. We acknowledge that there is the potential for misinterpretation
of the scatterplots we present in the current study, especially given their similarity in
form, but not purpose to bubble charts. 

## Future Work

At present, we have confirmed the potential for both point contrast and size 
manipulations to influence participants' perceptions of correlation in scatterplots,
each to varying degrees. It is also clear that these manipulations are not necessary,
and may be making perception worse, at certain values of *r*. We will therefore
investigate the effect of manipulating *both* point size and contrast on correlation 
estimation, and will introduce a parameter to control the strength of this 
family of manipulations according to the objective *r* value itself. 
More qualitative work is needed to both address any misinterpretation
of our adjusted scatterplots and to provide further insights into strategies 
participants may use. We have also demonstrated that our
experimental framework \cite{strain_2023} is transferable to other visual features
of plot design, and in the future could be applied to test other chart types or
statistical summaries.

# References {-}